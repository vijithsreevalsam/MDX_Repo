{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#template matching of UNO card\n",
    "\n",
    "def returnTemplate():\n",
    "    \n",
    "        import cv2\n",
    "        import numpy as np\n",
    "\n",
    "        #template loading\n",
    "        template_imges={\n",
    "            'class_0':cv2.imread(r'dataset\\Red_0.jpg',0),\n",
    "            'class_1':cv2.imread(r'dataset\\Red_1.jpg',0),\n",
    "            'class_2':cv2.imread(r'dataset\\Red_2.jpg',0),\n",
    "            'class_3':cv2.imread(r'dataset\\Red_3.jpg',0),\n",
    "            'class_4':cv2.imread(r'dataset\\Red_4.jpg',0),\n",
    "            'class_5':cv2.imread(r'dataset\\Red_5.jpg',0),\n",
    "            'class_6':cv2.imread(r'dataset\\Red_6.jpg',0),\n",
    "            'class_7':cv2.imread(r'dataset\\Red_7.jpg',0),\n",
    "            'class_8':cv2.imread(r'dataset\\Red_8.jpg',0),\n",
    "            'class_9':cv2.imread(r'dataset\\Red_9.jpg',0),\n",
    "            'class_Draw_2':cv2.imread(r'dataset\\Red_Draw_2.jpg',0),\n",
    "            'class_reverse':cv2.imread(r'dataset\\Red_Reverse.jpg',0),\n",
    "            'class_Skip':cv2.imread(r'dataset\\Red_Skip.jpg',0),\n",
    "            'class_Wild_draw_4':cv2.imread(r'dataset\\Wild_Draw_4.jpg',0),\n",
    "            'class_Wild':cv2.imread(r'dataset\\Wild.jpg',0),\n",
    "            \n",
    "            'class_0_1':cv2.imread(r'dataset\\web_cam_20240421_124122.png',0),\n",
    "            'class_1_1':cv2.imread(r'dataset\\web_cam_20240421_124420.png',0),\n",
    "            'class_2_1':cv2.imread(r'dataset\\2 (5).jpeg',0),\n",
    "            'class_3_1':cv2.imread(r'dataset\\3 (4).jpeg',0),\n",
    "            'class_4_1':cv2.imread(r'dataset\\4 (4).jpeg',0),\n",
    "            'class_5_1':cv2.imread(r'dataset\\5 (10).jpeg',0),\n",
    "            'class_6_1':cv2.imread(r'dataset\\6 (9).jpeg',0),\n",
    "            'class_7_1':cv2.imread(r'dataset\\7 (4).jpeg',0),\n",
    "            'class_8_1':cv2.imread(r'dataset\\8 (5).jpeg',0),\n",
    "            'class_9_1':cv2.imread(r'dataset\\9 (1).jpeg',0),\n",
    "            'class_Draw_2_1':cv2.imread(r'dataset\\draw2 (3).jpeg',0),\n",
    "            'class_Skip_1':cv2.imread(r'dataset\\skip (3).jpeg',0),\n",
    "            'class_reverse_1':cv2.imread(r'dataset\\reverse (5).jpeg',0),\n",
    "            'class_Wild_draw_4_1':cv2.imread(r'dataset\\wild4 (4).jpeg',0),\n",
    "            'class_Wild_1':cv2.imread(r'dataset\\wild (3).jpeg',0),\n",
    "            \n",
    "             'class_0_2':cv2.imread(r'dataset\\0 (8).jpeg',0),\n",
    "            'class_1_2':cv2.imread(r'dataset\\web_cam_20240421_124427.png',0),\n",
    "            'class_2_2':cv2.imread(r'dataset\\2 (7).jpeg',0),\n",
    "            'class_3_2':cv2.imread(r'dataset\\web_cam_20240421_124941.png',0),\n",
    "            'class_4_2':cv2.imread(r'dataset\\4 (7).jpeg',0),\n",
    "            'class_5_2':cv2.imread(r'dataset\\5 (7).jpeg',0),\n",
    "            'class_6_2':cv2.imread(r'dataset\\6 (3).jpeg',0),\n",
    "            'class_7_2':cv2.imread(r'dataset\\7 (7).jpeg',0),\n",
    "            'class_8_2':cv2.imread(r'dataset\\8 (7).jpeg',0),\n",
    "            'class_9_2':cv2.imread(r'dataset\\9 (3).jpeg',0),\n",
    "            'class_Draw_2_2':cv2.imread(r'dataset\\draw2 (7).jpeg',0),\n",
    "            'class_Skip_2':cv2.imread(r'dataset\\skip (9).jpeg',0),\n",
    "            'class_reverse_2':cv2.imread(r'dataset\\reverse (9).jpeg',0),\n",
    "            'class_Wild_draw_4_2':cv2.imread(r'dataset\\wild4 (6).jpeg',0),\n",
    "            'class_Wild_2':cv2.imread(r'dataset\\wild (10).jpeg',0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        }\n",
    "        \n",
    "        return template_imges\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(returnTemplate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Template matching \n",
    "###### cv2.templatemathing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcition to find matching template\n",
    "def detect_template(img):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    img2Gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    max_score=-np.inf\n",
    "    best_match_class=None\n",
    "    #template loading\n",
    "    template_imges=returnTemplate()\n",
    "    for class_name ,temp_image in template_imges.items():\n",
    "        resize_temp=cv2.resize(temp_image,(256,256))\n",
    "        resize_orginal=cv2.resize(img2Gray,(256,256))\n",
    "        result=cv2.matchTemplate(resize_orginal,resize_temp,cv2.TM_CCOEFF_NORMED)\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "        if max_val>max_score:\n",
    "            max_score=max_val\n",
    "            best_match_class=class_name\n",
    "    #print(f\"Best matching one is {best_match_class},score:{max_score}\") \n",
    "    \n",
    "    return best_match_class\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfMatchIngMethod(img):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    # Load the query image (UNO card image)\n",
    "    #query_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    best_match_class=None\n",
    "\n",
    "    \n",
    "       \n",
    "\n",
    "    # Initialize the ORB (Oriented FAST and Rotated BRIEF) detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Initialize the BFMatcher (Brute-Force Matcher)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "    threshold=0.8\n",
    "\n",
    "    # Extract keypoints and descriptors from the query image\n",
    "    query_keypoints, query_descriptors = orb.detectAndCompute(img, None)\n",
    "    template_imges=returnTemplate()\n",
    "    # Iterate over template images and labels\n",
    "    for class_name,temp_image in template_imges.items():\n",
    "        # Extract keypoints and descriptors from the template image\n",
    "        template_keypoints, template_descriptors = orb.detectAndCompute(temp_image, None)\n",
    "        \n",
    "        # Match keypoints between the query image and the template image\n",
    "        matches = bf.knnMatch(query_descriptors, template_descriptors, k=2)\n",
    "        \n",
    "        # Apply ratio test to select good matches\n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.75 * n.distance:\n",
    "                good_matches.append(m)\n",
    "        \n",
    "        # Print the number of good matches for each template image\n",
    "        #print(f\"Template : {len(good_matches)} good matches\")\n",
    "        \n",
    "        # You can further process the matches or use them for classification\n",
    "        \n",
    "        # Example: Calculate the percentage of good matches\n",
    "        if len(query_keypoints) != 0:\n",
    "            \n",
    "            percentage_good_matches = len(good_matches) / len(query_keypoints) * 100\n",
    "            #print(f\"Percentage of good matches: {percentage_good_matches}%\")\n",
    "        else: percentage_good_matches=0    \n",
    "        # Example: If percentage of good matches is above a threshold, classify as the corresponding label\n",
    "        if percentage_good_matches > threshold:\n",
    "            threshold=percentage_good_matches\n",
    "            \n",
    "            best_match_class=class_name\n",
    "            #print(f\"percentclass:{best_match_class}\")\n",
    "    print(threshold)  \n",
    "    if (threshold > 1.0 or threshold <1.5):\n",
    "        return best_match_class\n",
    "    else:\n",
    "        best_match_class='no class'\n",
    "        return best_match_class\n",
    "         \n",
    "    #print(f\"Detected class in BfMatcher: {best_match_class}\")\n",
    "    \n",
    "    #return best_match_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Color detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to detect dominant color and draw rectangle around it\n",
    "def detect_and_draw(frame):\n",
    "    # Convert frame to HSV color space\n",
    "    hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Define range of colors to detect\n",
    "    lower_red = np.array([0, 100, 100])\n",
    "    upper_red = np.array([10, 255, 255])\n",
    "    \n",
    "    lower_green = np.array([50, 100, 100])\n",
    "    upper_green = np.array([70, 255, 255])\n",
    "    \n",
    "    lower_blue = np.array([110, 100, 100])\n",
    "    upper_blue = np.array([130, 255, 255])\n",
    "\n",
    "    lower_yellow = np.array([20, 100, 100])\n",
    "    upper_yellow = np.array([40, 255, 255])\n",
    "\n",
    "    # Define colors for drawing rectangles\n",
    "    color_red = (0, 0, 255)   # Red\n",
    "    color_green = (0, 255, 0)  # Green\n",
    "    color_blue = (255, 0, 0)   # Blue\n",
    "    color_yellow = (0, 255, 255)  # Yellow\n",
    "\n",
    "    # Threshold the HSV image to get binary masks for each color\n",
    "    mask_red = cv2.inRange(hsv_frame, lower_red, upper_red)\n",
    "    mask_green = cv2.inRange(hsv_frame, lower_green, upper_green)\n",
    "    mask_blue = cv2.inRange(hsv_frame, lower_blue, upper_blue)\n",
    "    mask_yellow = cv2.inRange(hsv_frame, lower_yellow, upper_yellow)\n",
    "\n",
    "    # Find contours in each mask\n",
    "    contours_red, _ = cv2.findContours(mask_red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_green, _ = cv2.findContours(mask_green, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_blue, _ = cv2.findContours(mask_blue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_yellow, _ = cv2.findContours(mask_yellow, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    \n",
    "       # Count non-zero pixels in each mask (area of each color)\n",
    "    area_red = cv2.countNonZero(mask_red)\n",
    "    area_green = cv2.countNonZero(mask_green)\n",
    "    area_blue = cv2.countNonZero(mask_blue)\n",
    "    area_yellow = cv2.countNonZero(mask_yellow)\n",
    "\n",
    "    # Determine dominant color based on area\n",
    "    dominant_color = None\n",
    "    if area_red > area_green and area_red > area_blue and area_red > area_yellow:\n",
    "        dominant_color = \"Red\"\n",
    "    elif area_green > area_red and area_green > area_blue and area_green > area_yellow:\n",
    "        dominant_color = \"Green\"\n",
    "    elif area_blue > area_red and area_blue > area_green and area_blue > area_yellow:\n",
    "        dominant_color = \"Blue\"\n",
    "    elif area_yellow > area_red and area_yellow > area_green and area_yellow > area_blue:\n",
    "        dominant_color = \"Yellow\"\n",
    "    else:\n",
    "        dominant_color='No color'\n",
    "        \n",
    "\n",
    "    # Draw rectangles around detected areas\n",
    "    draw_rectangles(frame, contours_red, color_red)\n",
    "    draw_rectangles(frame, contours_green, color_green)\n",
    "    draw_rectangles(frame, contours_blue, color_blue)\n",
    "    draw_rectangles(frame, contours_yellow, color_yellow)\n",
    "    \n",
    "    return dominant_color\n",
    "\n",
    "# Function to draw rectangles around contours\n",
    "def draw_rectangles(frame, contours, color):\n",
    "    for contour in contours:\n",
    "        # Get bounding box coordinates and dimensions\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Draw rectangle around contour\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide label name for CNN model\n",
    "class_names=['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9', 'class_draw2', 'class_reverse', 'class_skip', 'class_wild', 'class_wild4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatchingClass(className):\n",
    "    \n",
    "    if className == 'class_0' or className=='class_0_1' or className=='class_0_2':\n",
    "        return 'card 0'\n",
    "    elif className=='class_1' or className =='class_1_1' or className=='class_1_2':\n",
    "        return 'card 1'\n",
    "    elif className =='class_2' or className =='class_2_1' or className=='class_2_2':\n",
    "        return 'card 2'\n",
    "    elif className =='class_3' or className =='class_3_1' or className=='class_3_2':\n",
    "        return 'card 3'\n",
    "    elif className =='class_4' or className =='class_4_1' or className=='class_4_2':\n",
    "        return 'card 4'\n",
    "    elif className =='class_5' or className =='class_5_1' or className=='class_5_2':\n",
    "        return 'card 5'\n",
    "    elif className =='class_6' or className =='class_6_1' or className=='class_6_2':\n",
    "        return 'card 6'\n",
    "    elif className =='class_7' or className =='class_7_1' or className=='class_7_2':\n",
    "        return 'card 7'\n",
    "    elif className =='class_8' or className =='class_8_1' or className=='class_8_2':\n",
    "        return 'card 8'\n",
    "    elif className =='class_9' or className =='class_9_1' or className=='class_9_2':\n",
    "        return 'card 9'\n",
    "    elif className =='class_Draw_2' or className =='class_Draw_2_1' or className =='class_Draw_2_2':\n",
    "        return 'card Draw 2'\n",
    "    elif className =='class_reverse' or className =='class_reverse_1' or className=='class_reverse_2':\n",
    "        return 'card reverse'\n",
    "    elif className =='class_Skip' or className =='class_Skip_1' or className=='class_Skip_2':\n",
    "        return 'card skip'\n",
    "    elif className =='class_Wild_draw_4' or className =='class_Wild_draw_4_1' or className=='class_Wild_draw_4_2':\n",
    "        return 'card Wild draw 4'\n",
    "    \n",
    "    elif className =='class_Wild' or className =='class_Wild_1' or className=='class_Wild_2':\n",
    "        return 'card Wild '\n",
    "    else: return 'No match'\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEdgedetection(img):\n",
    "    import matplotlib.pyplot as plt\n",
    "    img_rgb=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    guss_blur=cv2.GaussianBlur(img_rgb,(7,7),1)\n",
    "    object_only=img_rgb.copy()\n",
    "    gray_img=cv2.cvtColor(guss_blur,cv2.COLOR_BGR2GRAY)\n",
    "    edges=cv2.Canny(gray_img,50,150)\n",
    "    # Find contours of the edges to create a mask\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "  \n",
    "    if contours:\n",
    "        max_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(max_contour)\n",
    "        if cv2.contourArea(max_contour) > 5000:\n",
    "            cv2.rectangle(img_rgb, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "            object_only = img_rgb[y:y+h, x:x+w]\n",
    "            #cv2.imshow('My Smart Scanner', object_only)\n",
    "        \n",
    "    #cv2.imshow('cany image',mask)\n",
    "    #print(mask.shape)\n",
    "    #print   (cv2.resize(mask,(256,256)))\n",
    "    #cv2.waitKey(2)\n",
    "    \n",
    "    \n",
    "    return object_only,img_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image procossing for video capture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8169014084507045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025DBC323600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000025DBC323600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step\n",
      "prdiction is class_draw2\n",
      "confidence: 0.3977515995502472\n",
      "prdiction is class_draw2\n",
      "3.2467532467532463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653ms/step\n",
      "prdiction is class_draw2\n",
      "confidence: 0.2529389560222626\n",
      "prdiction is class_draw2\n",
      "2.82258064516129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step\n",
      "prdiction is class_2\n",
      "confidence: 0.21196581423282623\n",
      "prdiction is class_2\n",
      "4.736842105263158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step\n",
      "prdiction is class_wild\n",
      "confidence: 0.9663876295089722\n",
      "prdiction is class_wild\n",
      "25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380ms/step\n",
      "prdiction is class_skip\n",
      "confidence: 0.21637818217277527\n",
      "prdiction is class_skip\n",
      "17.391304347826086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step\n",
      "prdiction is class_skip\n",
      "confidence: 0.2069079577922821\n",
      "prdiction is class_skip\n",
      "21.73913043478261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step\n",
      "prdiction is class_skip\n",
      "confidence: 0.37588587403297424\n",
      "prdiction is class_skip\n",
      "20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step\n",
      "prdiction is class_skip\n",
      "confidence: 0.1764686107635498\n",
      "prediction uncertain\n",
      "16.666666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step\n",
      "prdiction is class_skip\n",
      "confidence: 0.2065049111843109\n",
      "prdiction is class_skip\n",
      "10.526315789473683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 385ms/step\n",
      "prdiction is class_skip\n",
      "confidence: 0.19914986193180084\n",
      "prediction uncertain\n",
      "22.22222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 396ms/step\n",
      "prdiction is class_skip\n",
      "confidence: 0.24499568343162537\n",
      "prdiction is class_skip\n",
      "23.52941176470588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407ms/step\n",
      "prdiction is class_skip\n",
      "confidence: 0.5552839636802673\n",
      "prdiction is class_skip\n",
      "3.218390804597701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step\n",
      "prdiction is class_skip\n",
      "confidence: 0.26172420382499695\n",
      "prdiction is class_skip\n",
      "3.523035230352303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443ms/step\n",
      "prdiction is class_wild\n",
      "confidence: 0.49577462673187256\n",
      "prdiction is class_wild\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers,models\n",
    "from keras import Sequential\n",
    "model=models.Sequential()\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "cap = cv2.VideoCapture(1)  # Change to appropriate video source if needed\n",
    "##cap.set(cv2.CAP_PROP_FRAME_WIDTH, 256)\n",
    "#cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 256)\n",
    "while True:\n",
    "    # Read frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    # Detect and draw rectangles around dominant colors\n",
    "    color=''\n",
    "    \n",
    "    masked_img,org_image=getEdgedetection((frame))\n",
    "    \n",
    "    #color=detect_and_draw(masked_img)\n",
    "    templte_matched=detect_template(masked_img)\n",
    "    bfmatch_check=bfMatchIngMethod(masked_img)\n",
    "    \n",
    "    masked_img=cv2.cvtColor(masked_img,cv2.COLOR_BGR2RGB)\n",
    "    color=detect_and_draw(masked_img)\n",
    "    \n",
    "    \n",
    "    #resizing image\n",
    "    new_width = 256\n",
    "    new_height = 256\n",
    "    resized_image = tf.image.resize(frame, [new_height, new_width])\n",
    "    model=models.load_model('UNO_with_old_way_96_7_accuracy.h5')\n",
    "    prediction=model.predict(np.array([resized_image])/255)\n",
    "    index=np.argmax(prediction)\n",
    "    max_confidence=prediction[0][index]# get the probability \n",
    "    print(f\"prdiction is {class_names[index]}\")\n",
    "    print(f'confidence: {max_confidence}')\n",
    "    accuracy_threshould=0.2\n",
    "    if max_confidence>=accuracy_threshould:\n",
    "    \n",
    "\n",
    "        print(f\"prdiction is {class_names[index]}\")\n",
    "        text = class_names[index] + \" \" + color\n",
    "        cv2.putText(org_image,text,(10,400),cv2.FONT_HERSHEY_SIMPLEX,1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        print(\"prediction uncertain\")\n",
    "        text=color\n",
    "        \n",
    "    \n",
    "\n",
    "    if bfmatch_check is not None:\n",
    "        label=MatchingClass(bfmatch_check)\n",
    "        text = \"cv2.bfmatch \"+ label + \" \" + color\n",
    "        cv2.putText(org_image, text, (10, 250), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 38, 38), 2)\n",
    "    else:\n",
    "        text = color\n",
    "        \n",
    "    if templte_matched is not None:\n",
    "        label=MatchingClass(bfmatch_check)\n",
    "        text = \"Cv2_temp_matching \" + label + \" \" + color\n",
    "        cv2.putText(org_image, text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 38, 38), 2)\n",
    "    else:\n",
    "        text = color\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    org_image=cv2.cvtColor(org_image,cv2.COLOR_RGB2BGR)\n",
    "    # Display the frame\n",
    "    cv2.imshow('Card Detection', org_image)\n",
    "    #cv2.imshow('frame', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
